# Author
author: Jimmy Callin
email: jimmy.callin@gmail.com

base_dir: /Users/jimmy/dev/edu/master-thesis/

development_mode: false

train: false
test: dev_data
store_test_results: results/cnn.txt
model: logistic_regression

# Model

models:
    logistic_regression:
        epochs: 100
        batch_size: 1000
        store_path: models/logreg.ckpt



# Each extractor has a name from main.extractor_handlers, which points to a class
# where any additional params are used to initiate the extractor class.

feature_extraction:
    extractors:
        # -
        #     name: word2vec
        #     path: resources/GoogleNews-vectors-negative300.bin
        # -
        #     name: onehot
        #     vocab_indices_path: resources/conll16st-en-zh-dev-train_LDC2016E50/vocab.txt
        -
            name: random_vectors
            dimensionality: 300


# Resources
resources:
    training_data:
        name: conll16st-en-01-12-16-train
        path: resources/conll16st-en-zh-dev-train_LDC2016E50/conll16st-en-01-12-16-train/relations.json
        max_hierarchical_level: 1
        max_words_in_sentence: 20
        classes: ["['Temporal']", "['Expansion']", "['Contingency']", "['Comparison']", "['EntRel']"]
        separate_dual_classes: true
    dev_data:
        name: conll16st-en-01-12-16-dev
        path: resources/conll16st-en-zh-dev-train_LDC2016E50/conll16st-en-01-12-16-dev/relations.json
        max_hierarchical_level: 1
        max_words_in_sentence: 20
        classes: ["['Temporal']", "['Expansion']", "['Contingency']", "['Comparison']", "['EntRel']"]
        separate_dual_classes: true

# Logging
logging:
    version: 1
    disable_existing_loggers: false
    formatters:
        default:
            format: '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'
    handlers:
        console:
            class: logging.StreamHandler
            stream: ext://sys.stdout
            formatter: default
            level: DEBUG
        file:
            class: logging.FileHandler
            filename: main.log
            formatter: default
            level: DEBUG
    root:
        handlers: [console, file]
        level: DEBUG

# Deployment process
deploy:
# Download these files to their location
    download:
        -
            name: brown_clusters
            from:
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/README.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c100-freq1.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c320-freq1.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c1000-freq1.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c3200-freq1.txt
            to: resources/brown_clusters
    # Make sure these files are where they should be with correct SHA
    # These are not possible to download from urls,
    # and therefore has to be done manually
    check_exists:
        -
            name: word2vec
            path: resources/GoogleNews-vectors-negative300.bin
            shasum: df6bee5cbaa95ec7fa389bf666d14d4a9ff91484
