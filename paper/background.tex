\chapter{Background}

\subsection{Shallow Discourse Parsing}

[THIS IS HOW DISCOURSE PARSING HAS TRADITIONALLY BEEN DONE THROUGH THE WORKS OF RHETORICAL STRUCTURE AND SEGMENTED DISCOURSE REPRESENTATION THEORY].

[THIS IS WHAT SHALLOW DISCOURSE PARSING TRIES TO DO DIFFERENTLY].

The shallow property is realized by the lack of a hierarchical structure, and also by not requiring total coverage of all text units within a document. This stands in contrast to other discourse frameworks, with the purpose that [WHY?].

\subsubsection{Penn Discourse Treebank}

The Penn Discourse Treebank (PRASAD ET AL 2008) is a subset of the Penn Treebank annotated according to the framework as specified by WEBBER (2004).  It follows a lexically-grounded approach to annotation of discourse relations (WEBBER ET AL 2003), and adopts a theory-neutral approach to annotation. It makes no assumptions on higher level structures beyond the lexically constrained annotation and their sense relation.

The PDTB framework defines two arguments, \emph{Arg1} and \emph{Arg2}, which make up the two text units that are connected by a discourse relation. These two arguments may or may not be connected by a \emph{discourse connective}. If a discourse connective is present we call it an \emph{explicit} discourse relation -- otherwise the relation is \emph{implicit}. A discourse connective is generally one or more words that binds the arguments together, e.g. by a subordinate conjunction such as \emph{because}, \emph{and}, or \emph{however}. If an explicit connective is present, whichever argument it is syntactically connected to is defined as \emph{Arg1}, while the other is \emph{Arg2}.

\subsubsection{Discourse connectives}

\begin{table}[t]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Type     & Frequency & Ratio \\ \midrule
Explicit & 14722 & 45.25\%    \\
Implicit & 13156 & 40.44\%    \\
EntRel   & 4133  & 12.7\%     \\
AltLex   & 524   & 1.61\%     \\ \bottomrule
\end{tabular}
\caption{Discourse relation type distribution.}
\label{tbl:discourse-relation-type-distribution}
\end{table}
See Table \ref{tbl:discourse-relation-type-distribution} for discourse type distribution. See Table \ref{tbl:sense-hierarchy} for sense hierarchy.

\subsubsection{Explicit vs implicit discourse connectives} \label{sec:implexpl}

\input{figures/sense_hierarchy}

It is not always possible to consider implicit discourse relations simply as explicit relations with the connective removed.

\begin{exe}
\ex I want to go to New York, but I already booked a flight. (I am not going to New York.)\label{example:notny}
\ex I want to go to New York, so I already booked a flight. (I am going to New York.)\label{example:ny}
\end{exe}

Here, we have either a COMPARISON.Contrast, or a CONTINGENCY.Cause relation depending on what form the sense takes. If we were to remove the connective token, our linguistic intuition tells us to default for the meaning of Example \ref{example:ny}, that is, CONTINGENCY.Cause. How can we use this to our advantage when classifying implicit connectives? (Honest question, still don't know.)

\subsection{Why do we add discourse markers?}

Asr, Fatemeh Torabi, and Vera Demberg. “Discourse Expectations and Implicitness of (Causal) Discourse Relations.” Accessed October 27, 2016. http://www.coli.uni-saarland.de/~fatemeh/AMLaP2012_abstract.pdf.



\section{Related work}


\section{The CoNLL-2016 Shared Task on Shallow Discourse Parsing}

At CoNLL 2015 they introduced a shared task on \emph{Shallow Discourse Parsing}. In this task, a system is fed a piece of newswire text as input and returns discourse relations similar to our previous examples. Such a system needs to locate both explicit or implicit discourse relations, identify the spans of its two relations, and finally predict the sense of the discourse connective.


The shared task for the CoNLL 2016 continues upon the work of last year's shared task while introducing Chinese as an alternative evaluation language. A new component is the subtask of \emph{sense classification} where correct argument dependencies and connective are already given, leaving out the sense of the discourse connective. The purpose of this is to allow more focus to be put into solely studying the properties of discourse connectives without having to worry about the pure parsing.

The work in this thesis is built generally upon the CoNLL-2016 shared task, and particularly on the subtask of sense classification.

\subsection{Findings of CoNLL-2016 Shared Task}

The submitted systems for CoNLL 2016 saw a clear trend toward neural network based architectures.
