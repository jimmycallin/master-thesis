# Author
author: Jimmy Callin
email: jimmy.callin@gmail.com

base_dir: /Users/jimmy/dev/edu/master-thesis/

development_mode: true

train: training_data
test: dev_data
store_test_results: results/cnn.txt
model_name: nn_baseline
# stored_model_path: None

# Model
model:
    name: logistic_regression
    batch_size: 500
    epochs: 10
    #store_path: models/logreg.bin


models:
    nn_baseline:
        batch_size: 10
        hack_max_len: 25
        hidden_dim: 100
        num_epochs: 1
        optimizer: GradientDescentOptimizer
        learning_rate: 0.01
        dont_train_embeddings: true




# Each extractor has a name from main.extractor_handlers, which points to a class
# where any additional params are used to initiate the extractor class.

feature_extraction:
    extractors:
        -
            name: word2vec
            path: resources/GoogleNews-vectors-negative300.bin


# Resources
resources:
    training_data:
        name: conll16st-en-01-12-16-train
        path: resources/conll16st-en-zh-dev-train_LDC2016E50/conll16st-en-01-12-16-train/relations.json
        max_words_in_sentence: 20
    dev_data:
        name: conll16st-en-01-12-16-dev
        path: resources/conll16st-en-zh-dev-train_LDC2016E50/conll16st-en-01-12-16-dev/relations.json
        max_words_in_sentence: 20

# Logging
logging:
    version: 1
    disable_existing_loggers: false
    formatters:
        default:
            format: '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'
    handlers:
        console:
            class: logging.StreamHandler
            stream: ext://sys.stdout
            formatter: default
            level: DEBUG
        file:
            class: logging.FileHandler
            filename: main.log
            formatter: default
            level: DEBUG
    root:
        handlers: [console, file]
        level: DEBUG

# Deployment process
deploy:
# Download these files to their location
    download:
        -
            name: brown_clusters
            from:
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/README.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c100-freq1.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c320-freq1.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c1000-freq1.txt
                - http://metaoptimize.s3.amazonaws.com/brown-clusters-ACL2010/brown-rcv1.clean.tokenized-CoNLL03.txt-c3200-freq1.txt
            to: resources/brown_clusters
    # Make sure these files are where they should be with correct SHA
    # These are not possible to download from urls,
    # and therefore has to be done manually
    check_exists:
        -
            name: word2vec
            path: resources/GoogleNews-vectors-negative300.bin
            shasum: df6bee5cbaa95ec7fa389bf666d14d4a9ff91484
